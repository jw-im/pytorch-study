{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8장. 컨볼루션을 활용한 일반화\n",
    "\n",
    "8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이전 코드 불러오기\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3) #인풋채널수, 아웃풋 채널수\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0)) #0번쨰 차원이 batch로\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#텐서오옹 텐서처리된 이미지를 이미지로 출력하는거\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.2 컨볼루션으로 피처 찾아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bias 0 으로 제거\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "\n",
    "#가중치에 상수 넣어 출력에서의 픽셀이 이웃픽셀에 대한 평균 가지게~(by conv.weight.fill_())\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#커널 바꿔보기\n",
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.3 깊이와 풀링으로 한 단계 더 인식하기\n",
    "-----------\n",
    "\n",
    "이미지 내 그림이 크면, 신경망이 패턴인식할때 '더 큰 커널 사용'\n",
    "커널 넘 크면 컨볼루션 장점 사라짐\n",
    "컨볼루션 쌓으면서 다운샘플링하는 방법은: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<큰 이미지에서 작은 이미지로: 다운샘플링>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#맥스풀링\n",
    "pool = nn.MaxPool2d(2) #이미지를 절반으로 줄임\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.4 우리의 신경망에 적용하기\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), #(채널수, 독립적인 피처, .., ..)\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1), #다운샘플링된 이미지, 높은 수준의 피처\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파라미터 갯수세기\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3 nn.Module 서브클래싱하기\n",
    "----\n",
    "\n",
    "8.3.1 nn.Module로 정의된 우리의 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Net class를 인스턴스화함. \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): #__int__ : 클래스 생성자\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2) ##2*2\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32) #입력크기 8*8*8, 출력크기 32\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "#순전파\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8) \n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0427, 0.0202]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3.3 함수형 API\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) #2 = 2*2 커널\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1720, 0.1507]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4 우리가 만든 컨볼루션 신경망 훈련시키기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련루프 완벽 구성 단계\n",
    "import datetime #날짜, 시간 연산 쉽게 수행하는 기본 라이브러리\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader): #인자 다섯개~~\n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        loss_train = 0.0 #변수 초기화\n",
    "        for imgs, labels in train_loader:  \n",
    "            \n",
    "            outputs = model(imgs) #model에 이미지 전달\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)  \n",
    "\n",
    "            optimizer.zero_grad()  #옵티마이저의 그래디언트 0으로 초기화\n",
    "            \n",
    "            loss.backward()  #손실의 그래디언트 계산\n",
    "            \n",
    "            optimizer.step() #옵티마이저로 매개변수 업데이트\n",
    "\n",
    "            loss_train += loss.item()  #현재 미니배치의 손실을 loss_train변수에 더함. / .item(): 텐서 to 스칼라\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 09:32:35.308198 Epoch 1, Training loss 0.5790962628118551\n",
      "2024-04-05 09:33:18.802368 Epoch 10, Training loss 0.32671168285190677\n",
      "2024-04-05 09:34:00.566184 Epoch 20, Training loss 0.2928355387441672\n",
      "2024-04-05 09:34:47.509333 Epoch 30, Training loss 0.2706265791681162\n",
      "2024-04-05 09:35:18.131595 Epoch 40, Training loss 0.2548033180320339\n",
      "2024-04-05 09:35:50.679917 Epoch 50, Training loss 0.23745034720487657\n",
      "2024-04-05 09:36:25.255780 Epoch 60, Training loss 0.22348935593655156\n",
      "2024-04-05 09:37:00.579051 Epoch 70, Training loss 0.2088231583404693\n",
      "2024-04-05 09:37:33.877013 Epoch 80, Training loss 0.1915418425942682\n",
      "2024-04-05 09:38:08.980566 Epoch 90, Training loss 0.17861243546198888\n",
      "2024-04-05 09:38:42.164270 Epoch 100, Training loss 0.16461306530389058\n"
     ]
    }
   ],
   "source": [
    "#정의 후 training_loop로 모델 학습\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)  # DataLoader 클래스\n",
    "\n",
    "model = Net()  #위에서 정의한거\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #학습률 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()   \n",
    "\n",
    "training_loop(  \n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "##이건 손실값으로 정확도 측정한거고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4.1 정확도 측정\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.89\n",
      "Accuracy val: 0.85\n"
     ]
    }
   ],
   "source": [
    "#손실값보다 해석하기 좋은 정확도 측정법\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]: \n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for imgs, labels in loader: #배치단위로 이미지, 레이블 가져옴\n",
    "                outputs = model(imgs) \n",
    "                _, predicted = torch.max(outputs, dim=1) #_: 최대값 자체 / predicted : 예측된 클래스(=최대값의 인덱스)\n",
    "                total += labels.shape[0]  #현재 배치에 있는 이미지 갯수 추가(for총 처리 갯수 추적)\n",
    "                correct += int((predicted == labels).sum())  #== 불리언 / 맞는거 .sum으로 계산/int로 정수로 변환해 갯수 계산\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4.2 모델을 저장하고 불러오기\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net() \n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4.3 GPU에서 훈련시키기\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4050 Laptop GPU'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.5.1 메모리 용량 늘리기: 너비\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self): #클래스의 초기화 메서드\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) #(입력채널, 출력채널)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32) #(입력크기, 출력 개수)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델은 GPU에 있는데 데이터는 CPU에 있다는데.. 어떻게 해결하져"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m validate(model, train_loader, val_loader)\n",
      "Cell \u001b[1;32mIn[49], line 9\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m      6\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;66;03m#변수 초기화\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:  \n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#model에 이미지 전달\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)  \n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m#옵티마이저의 그래디언트 초기화\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jw\\miniconda3\\envs\\study2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jw\\miniconda3\\envs\\study2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 10\u001b[0m, in \u001b[0;36mNetWidth.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jw\\miniconda3\\envs\\study2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jw\\miniconda3\\envs\\study2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jw\\miniconda3\\envs\\study2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jw\\miniconda3\\envs\\study2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "model = NetWidth().to(device=device) #GUP실행\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.5.2 모델이 수렴하고 일반화하도록 돕는 방법: 정규화\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<가중치 패널티>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "#손실함수에 _norm더해서 가중치 패널치 적용함. \n",
    "            l2_lambda = 0.001 #정규화 강도를 조절하는 하이퍼파라미터\n",
    "            l2_norm = sum(p.pow(2.0).sum() #_norm : 모든 파라미터의 제곱합 / p.pow = p의 각 원소를 ()안의 지수로 거듭제곱\n",
    "                          for p in model.parameters()) \n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-02 21:25:32.142205 Epoch 1, Training loss 0.6198746459499286\n",
      "2024-04-02 21:25:40.892631 Epoch 10, Training loss 0.3628305968395464\n",
      "2024-04-02 21:25:50.716108 Epoch 20, Training loss 0.32362234905646864\n",
      "2024-04-02 21:26:00.434486 Epoch 30, Training loss 0.2992090948258236\n",
      "2024-04-02 21:26:10.580536 Epoch 40, Training loss 0.28116252695678906\n",
      "2024-04-02 21:26:20.977945 Epoch 50, Training loss 0.26707421727241226\n",
      "2024-04-02 21:26:31.216001 Epoch 60, Training loss 0.25404627929637386\n",
      "2024-04-02 21:26:41.670726 Epoch 70, Training loss 0.24203938117642312\n",
      "2024-04-02 21:26:51.756573 Epoch 80, Training loss 0.23109726152222626\n",
      "2024-04-02 21:27:01.956099 Epoch 90, Training loss 0.22129109535057834\n",
      "2024-04-02 21:27:12.198106 Epoch 100, Training loss 0.21251973580972405\n",
      "Accuracy train: 0.91\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<드랍아웃>: 훈련 반복할떄마다 신경망의 뉴런 출력을 랜덤하게 0으로 만드는것 _ 오버피팅 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4) #40%확률로 픽셀이 드롭아웃됨(학습중에만)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4) \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2) #view : 텐서 모양 변경 \n",
    "        #-1: 첫번째 차원. 텐서의 총 요소 수 유지하면서 다른 차원 크기에 따라 첫번째 차원의 크기를 자동으로 조정)\n",
    "        # 8 * 8 * self.n_chans1 / 2: 두번쨰 차원의 크기. self.n_chans1 : 채널수 / //2 : 결과를 2로 나눔\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<배치 정규화>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__() #super() : 승계\n",
    "        self.n_chans1 = n_chans1 \n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1) #정규화 모듈 :batchnormwd ~~\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
